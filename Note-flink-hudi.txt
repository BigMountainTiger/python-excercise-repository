-----------------------------------------------------------------------

package com.song.examples.kafkaconfluent;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class ReadFromConfluent {

	public static void main(String[] args) throws Exception {

		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		KafkaSource<String> kafaka = kafaka();
		DataStream<String> stm = env.fromSource(kafaka, WatermarkStrategy.noWatermarks(), "Kafka Source");

		DataStream<String> stm1 = stm.map(new MapFunction<String, String>() {
			@Override
			public String map(String value) {
				return "Mapped - " + value;
			}
		});

		stm1.print();

		System.out.println("Start execution");
		env.execute("Flink Streaming Java API Skeleton");
	}

	private static KafkaSource<String> kafaka() {

		final String SERVER = System.getenv("SERVER");
		final String CLUSTER_ID = System.getenv("CLUSTER_ID");
		final String KEY = System.getenv("KEY");
		final String SECRET = System.getenv("SECRET");
		final String TOPIC = System.getenv("TOPIC");

		System.out.println(SERVER);
		System.out.println(CLUSTER_ID);
		System.out.println(KEY);
		System.out.println(SECRET);
		System.out.println(TOPIC);

		// The API Key & secret is the only thing needed to access the topic
		String sasl_jaas_config = "org.apache.kafka.common.security.plain.PlainLoginModule required " +
				"username='" + KEY + "' " +
				"password='" + SECRET + "';";

		KafkaSource<String> source = KafkaSource.<String>builder()
				.setTopics(TOPIC)
				.setProperty("bootstrap.servers", SERVER)
				.setProperty("security.protocol", "SASL_SSL")
				.setProperty("sasl.jaas.config", sasl_jaas_config)
				.setProperty("sasl.mechanism", "PLAIN")
				.setStartingOffsets(OffsetsInitializer.earliest())
				.setValueOnlyDeserializer(new SimpleStringSchema())
				.build();

		return source;
	}
}

-----------------------------------------------------------------------
Kafka docker

https://www.baeldung.com/ops/kafka-docker-setup

docker-compose.yml
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 22181:2181
  
  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 29092:29092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      

cluster-start

#!/bin/bash

cd .scripts/kafka/

docker-compose up -d


cluster-stop

#!/bin/bash

cd .scripts/kafka/

docker-compose down


topic-create

#!/bin/bash


kafka-topics.bat --bootstrap-server=localhost:29092 --topic song --create
kafka-topics.bat --bootstrap-server=localhost:29092 --list

topic-publish

#!/bin/bash

kafka-console-producer.bat -broker-list localhost:29092 -topic song

topic-consume

#!/bin/bash

kafka-console-producer.bat -broker-list localhost:29092 -topic song



-----------------------------------------------------------------------
Simple streaming

package example.song.com;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class StreamingJob_kafka {

	public static void main(String[] args) throws Exception {

		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		KafkaSource<String> kafaka = kafaka();
		DataStream<String> stm = env.fromSource(kafaka, WatermarkStrategy.noWatermarks(), "Kafka Source");

		DataStream<String> stm1 = stm.map(new MapFunction<String, String>() {
			@Override
			public String map(String value) {
				return "Mapped - " + value;
			}
		});

		stm1.print();

		System.out.println("Start execution");
		env.execute("Flink Streaming Java API Skeleton");
	}

	private static KafkaSource<String> kafaka() {
		String topic = "song";
		String brokers = "localhost:29092";

		KafkaSource<String> source = KafkaSource.<String>builder()
				.setBootstrapServers(brokers)
				.setTopics(topic)
				.setStartingOffsets(OffsetsInitializer.latest())
				.setValueOnlyDeserializer(new SimpleStringSchema())
				.build();

		return source;
	}

}

-----------------------------------------------------------------------
Text

package example.song.com;

import java.io.File;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringEncoder;
import org.apache.flink.connector.file.sink.FileSink;
import org.apache.flink.connector.file.src.FileSource;
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.FileUtils;

public class DataStreamJob_PlainText {

	public static void main(String[] args) throws Exception {
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		final String input = "./test-files/plain-text/input.txt";
		final String output = "./test-files/plain-text/output";

		final Path input_path = new Path(input);
		final Path output_path = new Path(output);

		FileUtils.deleteDirectory(new File(output));

		final FileSource<String> source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), input_path)
				.build();

		final FileSink<String> sink = FileSink
				.forRowFormat(output_path, new SimpleStringEncoder<String>("UTF-8"))
				.build();

		final DataStream<String> stm = env.fromSource(source, WatermarkStrategy.noWatermarks(), "file-source");

		DataStream<String> stm1 = stm.map(new MapFunction<String, String>() {
			@Override
			public String map(String value) {
				return "Mapped - " + value;
			}
		});

		stm1.print();
		stm1.sinkTo(sink);

		System.out.println("Start execution");
		env.execute("Flink Streaming");

	}
}




-----------------------------------------------------------------------
Kafka

package example.song.com;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class StreamingJob_kafka {

	public static void main(String[] args) throws Exception {

		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		KafkaSource<String> kafaka = kafaka();
		DataStream<String> stm = env.fromSource(kafaka, WatermarkStrategy.noWatermarks(), "Kafka Source");

		DataStream<String> stm1 = stm.map(new MapFunction<String, String>() {
			@Override
			public String map(String value) {
				return "Mapped - " + value;
			}
		});

		stm1.print();

		System.out.println("Start execution");
		env.execute("Flink Streaming Java API Skeleton");
	}

	private static KafkaSource<String> kafaka() {
		String topic = "song";
		String brokers = "localhost:29092";

		KafkaSource<String> source = KafkaSource.<String>builder()
				.setBootstrapServers(brokers)
				.setTopics(topic)
				.setStartingOffsets(OffsetsInitializer.latest())
				.setValueOnlyDeserializer(new SimpleStringSchema())
				.build();

		return source;
	}

}



-----------------------------------------------------------------------
Test

package example.song.com;

import java.text.MessageFormat;
import java.util.HashMap;
import java.util.Map;

import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.data.RowData;
import org.apache.hudi.configuration.FlinkOptions;
import org.apache.hudi.util.HoodiePipeline;

public class DataStreamJob_Hudi_read {

	public static void main(String[] args) throws Exception {
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		String test_name = "hudi";
		final String input = MessageFormat.format("/home/song/Sandbox/flink-example/sink-example/test-files/{0}/output",
				new Object[] { test_name });

		String targetTable = "t1";
		String basePath = input;

		Map<String, String> options = new HashMap<>();
		options.put(FlinkOptions.PATH.key(), basePath);

		HoodiePipeline.Builder builder = HoodiePipeline.builder(targetTable)
				.column("id INT")
				.column("name VARCHAR(100)")
				.pk("id")
				.options(options);

		DataStream<RowData> rowDataDataStream = builder.source(env);
		rowDataDataStream.print();

		System.out.println("Start execution");
		env.execute("Flink Streaming");

	}
}



------------------------------------------------------------------------
Read

package example.song.com;

import java.text.MessageFormat;
import java.util.HashMap;
import java.util.Map;

import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.data.RowData;
import org.apache.hudi.configuration.FlinkOptions;
import org.apache.hudi.util.HoodiePipeline;

public class DataStreamJob_Hudi_read {

	public static void main(String[] args) throws Exception {
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		String test_name = "hudi";
		final String input = MessageFormat.format("/home/song/Sandbox/flink-example/sink-example/test-files/{0}/output",
				new Object[] { test_name });

		String targetTable = "t1";
		String basePath = input;

		Map<String, String> options = new HashMap<>();
		options.put(FlinkOptions.PATH.key(), basePath);

		HoodiePipeline.Builder builder = HoodiePipeline.builder(targetTable)
				.column("id INT")
				.column("name VARCHAR(100)")
				.pk("id")
				.options(options);

		DataStream<RowData> rowDataDataStream = builder.source(env);
		rowDataDataStream.print();

		System.out.println("Start execution");
		env.execute("Flink Streaming");

	}
}


-------------------------------------------------------------------------
Write...

package example.song.com;

import java.text.MessageFormat;
import java.util.HashMap;
import java.util.Map;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.connector.file.src.FileSource;
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.data.GenericRowData;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.data.StringData;
import org.apache.hudi.configuration.FlinkOptions;
import org.apache.hudi.util.HoodiePipeline;
import org.json.JSONObject;

public class DataStreamJob_Hudi {

	public static void main(String[] args) throws Exception {
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		String test_name = "hudi";
		final String input = MessageFormat.format("./test-files/{0}/input", new Object[] { test_name });
		final String output = MessageFormat.format("/home/song/Sandbox/flink-example/sink-example/test-files/{0}/output", new Object[] { test_name });

		// FileUtils.deleteDirectory(new File(output));

		final FileSource<String> source = FileSource
				.forRecordStreamFormat(new TextLineInputFormat(), new Path(input))
				.build();

		final DataStream<String> stm = env.fromSource(source, WatermarkStrategy.noWatermarks(), "file-source");
		final DataStream<RowData> stm1 = stm.map(new MapFunction<String, RowData>() {
			@Override
			public RowData map(String value) {
				GenericRowData data = new GenericRowData(2);
				JSONObject o = new JSONObject(value);

				data.setField(0, o.getInt("id"));
				data.setField(1, StringData.fromString(o.getString("name")));

				return (RowData) data;
			}
		});

		stm1.print();

		String targetTable = "t1";
		String basePath = output;

		Map<String, String> options = new HashMap<>();
		options.put(FlinkOptions.PATH.key(), basePath);

		HoodiePipeline.Builder builder = HoodiePipeline.builder(targetTable)
				.column("id INT")
				.column("name VARCHAR(100)")
				.pk("id")
				// .partition("partition")
				.options(options);

		builder.sink(stm1, true);

		System.out.println("Start execution");
		env.execute("Flink Streaming");

	}
}


--------------------------------------------------------------------------------------

#!/bin/bash

# https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/

mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink \
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.16.0
      
-----------------------------------

package example.song.com;

import java.text.MessageFormat;
import java.util.HashMap;
import java.util.Map;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.connector.file.src.FileSource;
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.data.GenericRowData;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.data.StringData;
import org.apache.hudi.configuration.FlinkOptions;
import org.apache.hudi.util.HoodiePipeline;
import org.json.JSONObject;

public class DataStreamJob_Hudi {

	public static void main(String[] args) throws Exception {
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		String test_name = "hudi";
		final String input = MessageFormat.format("./test-files/{0}/input", new Object[] { test_name });
		final String output = MessageFormat.format("/home/song/Sandbox/flink-example/sink-example/test-files/{0}/output", new Object[] { test_name });

		// FileUtils.deleteDirectory(new File(output));

		final FileSource<String> source = FileSource
				.forRecordStreamFormat(new TextLineInputFormat(), new Path(input))
				.build();

		final DataStream<String> stm = env.fromSource(source, WatermarkStrategy.noWatermarks(), "file-source");
		final DataStream<RowData> stm1 = stm.map(new MapFunction<String, RowData>() {
			@Override
			public RowData map(String value) {
				GenericRowData data = new GenericRowData(2);
				JSONObject o = new JSONObject(value);

				data.setField(0, o.getInt("id"));
				data.setField(1, StringData.fromString(o.getString("name")));

				return (RowData) data;
			}
		});

		stm1.print();

		String targetTable = "t1";
		String basePath = output;

		Map<String, String> options = new HashMap<>();
		options.put(FlinkOptions.PATH.key(), basePath);

		HoodiePipeline.Builder builder = HoodiePipeline.builder(targetTable)
				.column("id INT")
				.column("name VARCHAR(100)")
				.pk("id")
				// .partition("partition")
				.options(options);

		builder.sink(stm1, true);

		System.out.println("Start execution");
		env.execute("Flink Streaming");

	}
}


----------------------------------

{"id": 1, "name": "Song Li"}
{"id": 2, "name": "George Bush"}
{"id": 3, "name": "Donald Trump"}
{"id": 4, "name": "Joe Biden"}
{"id": 5, "name": "Bill Clinton"}


----------------------------------

<project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<groupId>example.song.com</groupId>
	<artifactId>sink-example</artifactId>
	<version>1.0-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>Flink Sink Example</name>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<flink.version>1.16.0</flink.version>
		<target.java.version>11</target.java.version>
		<scala.binary.version>2.12</scala.binary.version>
		<junit.version>4.13.2</junit.version>
		<maven.compiler.source>${target.java.version}</maven.compiler.source>
		<maven.compiler.target>${target.java.version}</maven.compiler.target>
		<log4j.version>2.20.0</log4j.version>
	</properties>

	<repositories>
		<repository>
			<id>apache.snapshots</id>
			<name>Apache Development Snapshot Repository</name>
			<url>https://repository.apache.org/content/repositories/snapshots/</url>
			<releases>
				<enabled>false</enabled>
			</releases>
			<snapshots>
				<enabled>true</enabled>
			</snapshots>
		</repository>
	</repositories>

	<dependencies>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-streaming-java</artifactId>
			<version>${flink.version}</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-clients</artifactId>
			<version>${flink.version}</version>
			<scope>provided</scope>
		</dependency>

		<!-- flink-table -->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-common</artifactId>
			<version>1.16.1</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java</artifactId>
			<version>1.16.1</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java-bridge</artifactId>
			<version>1.16.1</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner_2.12</artifactId>
			<version>1.16.1</version>
			<scope>test</scope>
		</dependency>

		<!-- org.apache.hadoop -->
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>3.3.4</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-mapreduce-client-core</artifactId>
			<version>3.3.4</version>
		</dependency>

		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-main</artifactId>
			<version>3.3.4</version>
			<type>pom</type>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-connector-files</artifactId>
			<version>${flink.version}</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hudi</groupId>
			<artifactId>hudi-flink1.16-bundle</artifactId>
			<version>0.13.0</version>
		</dependency>

		<dependency>
			<groupId>org.json</groupId>
			<artifactId>json</artifactId>
			<version>20230227</version>
		</dependency>

		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>${junit.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-slf4j-impl</artifactId>
			<version>${log4j.version}</version>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-api</artifactId>
			<version>${log4j.version}</version>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-core</artifactId>
			<version>${log4j.version}</version>
			<scope>runtime</scope>
		</dependency>

		<!-- https://mvnrepository.com/artifact/log4j/log4j -->
		<dependency>
			<groupId>log4j</groupId>
			<artifactId>log4j</artifactId>
			<version>1.2.17</version>
		</dependency>


	</dependencies>

	<build>
		<plugins>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.1</version>
				<configuration>
					<source>${target.java.version}</source>
					<target>${target.java.version}</target>
				</configuration>
			</plugin>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<version>3.1.1</version>
				<executions>
					<execution>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
						<configuration>
							<createDependencyReducedPom>false</createDependencyReducedPom>
							<artifactSet>
								<excludes>
									<exclude>org.apache.flink:flink-shaded-force-shading</exclude>
									<exclude>com.google.code.findbugs:jsr305</exclude>
									<exclude>org.slf4j:*</exclude>
									<exclude>org.apache.logging.log4j:*</exclude>
								</excludes>
							</artifactSet>
							<filters>
								<filter>
									<artifact>*:*</artifact>
									<excludes>
										<exclude>META-INF/*.SF</exclude>
										<exclude>META-INF/*.DSA</exclude>
										<exclude>META-INF/*.RSA</exclude>
									</excludes>
								</filter>
							</filters>
							<transformers>
								<transformer
									implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer" />
								<transformer
									implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
									<mainClass>example.song.com.DataStreamJob</mainClass>
								</transformer>
							</transformers>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

		<pluginManagement>
			<plugins>
			</plugins>
		</pluginManagement>
	</build>
</project>
